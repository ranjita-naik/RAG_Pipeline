
# ğŸ“š **RAG Pipeline â€“ Production-Ready Retrieval-Augmented Generation System**

This repository contains a **production-grade Retrieval-Augmented Generation (RAG) system** built with:

- **Hybrid Retrieval** (BM25 + dense embeddings)
- **LLM-based Reranking** for improved context relevance
- **Configurable Chunking & Metadata Preservation**
- **FAISS Vector Indexing**
- **FastAPI Backend** for easy deployment
- **Streamlit Frontend** for interactive exploration
---

# ğŸ”¥ **Key Features**

### âœ… **1. Modular, Scalable Architecture**
The system is split into clean modules:

```
src/
  ingestion/     â†’ loaders, chunkers, embedding, vector index builder
  retrieval/     â†’ dense, hybrid, reranking, retrieval pipeline
  rag_chain.py   â†’ builds the final RAG pipeline
  api/           â†’ FastAPI service
app/
  streamlit_app.py â†’ user-friendly chat interface
```

Supports:
- Multi-stage retrieval
- Reranking
- Configurable vector stores
- Clean orchestration logic

---

### âœ… **2. Hybrid Retrieval**
Combines:

- **BM25 Lexical Retriever**
- **Dense Vector Retrieval (FAISS)**
- **Weighted score fusion (alpha blending)**

Benefits:
- Better recall
- Better robustness to keyword mismatch
- Stronger hallucination resistance

---

### âœ… **3. LLM-Based Reranking**
A lightweight reranker boosts precision using an LLM to score documents for the query.

Results in:
- More relevant chunks
- Fewer hallucinations
- Higher answer faithfulness

---

### âœ… **4. Configurable Chunking & Metadata**
Chunking preserves metadata such as:
- Start index
- Document IDs
- PDF page numbers

This enables:
- Reranking
- Citation-based answers
- Precise traceability

---

### âœ… **5. FastAPI Backend (Production Deployment)**
Expose the RAG pipeline as a REST API:

- `/ask` endpoint returns answer + sources
- Stateless and deployable on Docker, serverless, or VM
- Easy integration into existing products

---

### âœ… **6. Streamlit Chat UI (Optional Frontend)**
A simple web interface for:
- Asking questions
- Viewing retrieved context
- Testing pipeline variants (dense, hybrid, reranked)

---

# ğŸ§± **Project Structure**

```
.
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ config.py
â”‚   â”œâ”€â”€ rag_chain.py
â”‚   â”œâ”€â”€ ingestion/
â”‚   â”‚   â”œâ”€â”€ loaders.py
â”‚   â”‚   â”œâ”€â”€ chunker.py
â”‚   â”‚   â”œâ”€â”€ embedder.py
â”‚   â”‚   â”œâ”€â”€ vectorstore.py
â”‚   â”‚   â””â”€â”€ pipeline.py
â”‚   â”‚
â”‚   â”œâ”€â”€ retrieval/
â”‚   â”‚   â”œâ”€â”€ retriever_base.py
â”‚   â”‚   â”œâ”€â”€ dense_retriever.py
â”‚   â”‚   â”œâ”€â”€ hybrid_retriever.py
â”‚   â”‚   â”œâ”€â”€ reranker.py
â”‚   â”‚   â””â”€â”€ pipeline.py
â”‚   â”‚
â”‚   â”œâ”€â”€ api/
â”‚   â”‚   â””â”€â”€ fastapi_app.py
â”‚
â”œâ”€â”€ app/
â”‚   â””â”€â”€ streamlit_app.py
â”‚
â”œâ”€â”€ data/
â”‚   â””â”€â”€ *.pdf
â”œâ”€â”€ vectorstore/
â””â”€â”€ README.md
```

---

# âš™ï¸ **Installation**

### 1. Clone the repository

```bash
git clone https://github.com/ranjita-naik/RAG_Pipeline.git
cd RAG_Pipeline
```

### 2. Install dependencies

```bash
pip install -r requirements.txt
```

### 3. Set your OpenAI API key

```bash
export OPENAI_API_KEY="your_key_here"
```

---

# ğŸ—ï¸ **Building the Vector Index**

Place your PDFs in the `data/` folder.

Then run:

```bash
python -m src.ingestion.pipeline
```

This will:

- Load all PDFs
- Split into chunks
- Generate embeddings
- Build a FAISS vector index in `vectorstore/`

---

# ğŸ¤– **Running the RAG API (FastAPI)**

```bash
uvicorn src.api.fastapi_app:app --reload --port 8000
```

Make a request:

```bash
curl -X POST http://localhost:8000/ask   -H "Content-Type: application/json"   -d '{"question": "What is retrieval-augmented generation?"}'
```

---

# ğŸ’¬ **Running the Streamlit App**

```bash
streamlit run app/streamlit_app.py
```

---

# ğŸ§  **How Retrieval Works**

### **Stage 1: Lexical Search (BM25)**
Captures keyword-based relevance.

### **Stage 2: Dense Retrieval**
FAISS vector search using embeddings.

### **Stage 3: Score Fusion**
Blends BM25 + dense rankings:

```
final_score = Î± * dense + (1 - Î±) * bm25
```

### **Stage 4: LLM Reranking (Optional)**
Reorders top candidates using LLM scoring.

### **Stage 5: Context Assembly & Answer Generation**
Passes best documents into a deterministic LLM for answer generation.


